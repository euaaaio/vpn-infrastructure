---
# References:
#   - https://github.com/XTLS/Xray-core optimization guides
#   - https://gist.github.com/Nader-abdi/029532410f537ee499dc7fc3835bfccb

- name: Enable BBR congestion control (Google's high-performance algorithm)
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # BBR requires FQ (Fair Queue) qdisc instead of default fq_codel
    - { name: 'net.core.default_qdisc', value: 'fq' }
    # BBR significantly improves throughput and reduces latency vs cubic
    # Requires Linux kernel 4.9+ (we have 5.15/6.8)
    - { name: 'net.ipv4.tcp_congestion_control', value: 'bbr' }

- name: Enable TCP Fast Open for client and server
  ansible.posix.sysctl:
    name: net.ipv4.tcp_fastopen
    # 1 = client only, 2 = server only, 3 = both (we need server mode)
    # Reduces latency by sending data during initial TCP handshake
    value: '3'
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf

- name: Ensure TCP Window Scaling is enabled
  ansible.posix.sysctl:
    name: net.ipv4.tcp_window_scaling
    # Allows dynamic TCP window size adjustment for high-bandwidth connections
    # Should already be enabled, but we set it explicitly
    value: '1'
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf

- name: Configure network buffer sizes for high-load VPN
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # Socket buffers: 16MB max (can increase to 64MB if needed)
    # Fixed Caddy "UDP receive buffer size" warnings
    - { name: 'net.core.rmem_max', value: '16777216' }
    - { name: 'net.core.wmem_max', value: '16777216' }
    - { name: 'net.core.rmem_default', value: '2097152' }
    - { name: 'net.core.wmem_default', value: '2097152' }

- name: Configure TCP buffer tuning
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # TCP buffers: min 4KB, default 87KB/65KB, max 16MB
    - { name: 'net.ipv4.tcp_rmem', value: '4096 87380 16777216' }
    - { name: 'net.ipv4.tcp_wmem', value: '4096 65536 16777216' }

- name: Configure connection backlog for high traffic spikes
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # Maximum backlog of pending connections (already optimal at 4096)
    - { name: 'net.core.somaxconn', value: '4096' }
    # SYN backlog: queue size for half-open connections
    - { name: 'net.ipv4.tcp_max_syn_backlog', value: '8192' }
    # Network device backlog for incoming packets
    - { name: 'net.core.netdev_max_backlog', value: '16384' }

- name: Configure file descriptor limits
  ansible.posix.sysctl:
    name: fs.file-max
    # Maximum number of file handles (one connection = multiple fd)
    # Default is already at maximum, but we set explicitly
    value: '2097152'
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf

- name: Optimize TCP TIME_WAIT handling
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # Allow reuse of TIME_WAIT sockets for new connections
    # Value 2 = secure mode (modern kernels), 1 = classic mode
    - { name: 'net.ipv4.tcp_tw_reuse', value: '1' }
    # Reduce FIN timeout from 60s to 30s
    - { name: 'net.ipv4.tcp_fin_timeout', value: '30' }
    # Limit number of TIME_WAIT buckets
    - { name: 'net.ipv4.tcp_max_tw_buckets', value: '5000' }

- name: Increase conntrack table size for VPN server
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # Maximum tracked connections (16x increase from default)
    - { name: 'net.netfilter.nf_conntrack_max', value: '131072' }
    # Hash table buckets for conntrack
    - { name: 'net.netfilter.nf_conntrack_buckets', value: '32768' }

- name: Configure conntrack timeout (2 hours instead of 5 days)
  ansible.posix.sysctl:
    name: net.netfilter.nf_conntrack_tcp_timeout_established
    # Shorter timeout = faster cleanup of idle connections
    value: '7200'
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf

- name: Configure TCP keepalive optimization for VPN
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # Start probing after 10 min idle instead of 2 hours
    - { name: 'net.ipv4.tcp_keepalive_time', value: '600' }
    # Probe interval: 30 seconds
    - { name: 'net.ipv4.tcp_keepalive_intvl', value: '30' }
    # Number of failed probes before closing connection
    - { name: 'net.ipv4.tcp_keepalive_probes', value: '3' }

- name: Enable MTU probing and SYN cookies
  ansible.posix.sysctl:
    name: "{{ item.name }}"
    value: "{{ item.value }}"
    state: present
    reload: true
    sysctl_file: /etc/sysctl.conf
  loop:
    # Automatic MTU discovery to reduce packet fragmentation
    - { name: 'net.ipv4.tcp_mtu_probing', value: '1' }
    # SYN cookies: protection against SYN flood attacks
    - { name: 'net.ipv4.tcp_syncookies', value: '1' }
    # Local port range for outgoing connections
    - { name: 'net.ipv4.ip_local_port_range', value: '10000 65000' }
